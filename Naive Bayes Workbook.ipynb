{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d684a9a-ccbb-4710-92f9-e62ee4a2e97e",
   "metadata": {},
   "source": [
    "# Naive Bayes Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4108ee0c-bc19-4885-aacd-f021a481adfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "    <summary>README.md Copy</summary>\n",
    "This is an exercise to walk through the Naive Bayes algorithm. You will be walked through the basics of Bayes Theorem under the assumption you have no prior knowledge. We will then introduce you to Naive Bayes as a machine learning algorithm. <br><br>\n",
    "\n",
    "You will be asked to implement the algorithm from scratch. Please avoid the use of scikit-learn or other machine learning libraries for the implementation of naive bayes - pandas, numpy, or similar libraries are what we expect to see for your Python implementation. It is acceptable to use scikit-learn libraries (or other libraries) for evaluating your algorithm's correctness.<br>\n",
    "\n",
    "Your notebook will be evaluated on the following:\n",
    "<ul>\n",
    "    <li>Correctness</li>\n",
    "    <li>Legibility</li>\n",
    "    <li>Documentation</li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Please fork this repo and use the notebook within. When you have completed your notebook, present your submission by sharing your repo with us and sending us an email with a direct link. It is important that you demonstrate your ability to use GitHub as well as to complete the notebook assignment.<br>\n",
    "\n",
    "If you do not already have <a href=\"https://docs.anaconda.com/navigator/install/\">Anaconda Navigator</a> installed, you will need to do so. You can opt for a different setup of your python environment, but Anaconda is recommended for beginners. Anaconda comes by default with JupyterLab and JupyterNotebook both of which will allow you to open and edit the Python notebook. Anaconda includes a basic enviroment with the most used Python data science packages, but you may install additional packages to complete this excercise. You can search <a href=\"https://anaconda.org/\">https://anaconda.org</a> for terminal commands to install more packages or use the Anaconda GUI.<br>\n",
    "\n",
    "\n",
    "**While important, this assignment is not the sole candidate selection criteria.**  Please do not be discouraged if you are having difficulties understanding or implementing the assignment.  We are here to assist you with questions or concerns that come up while trying to complete it.  Do not hesitate to reach out to brooke.r.weborg@nasa.gov, meghan.bush@nasa.gov, and jeremiah.d.sims@nasa.gov <br>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df06baf-e920-4848-b5a7-4eb4e3485aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68f60c-2945-4e68-b2b6-05dcf8f2fce4",
   "metadata": {},
   "source": [
    "We will now do a brief introduction to the probability theory you need to understand to complete the notebook. A lot of this should be review, but in the case you may not have taken a probability and statistics class yet, the first half of this notebook will provide the information to understand what Bayes rule is before we ask you to code Naive Bayes. This section in not mandatory and can be skipped entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf6297-d0b5-4bbd-9fce-0c8c62b6c968",
   "metadata": {},
   "source": [
    "### Review of Probability Theory\n",
    "If you haven't taken an intro to statistics and probability class yet (or you need a refresher), and the following is very confusing for you, please check out this awesome and interactive reference from Brown University to get some foundational knowledge:<br>\n",
    "https://seeing-theory.brown.edu/basic-probability/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a51b3f-45f4-4c4f-ab6c-c00fda41d749",
   "metadata": {},
   "source": [
    "### Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc40794-9ba0-4da3-9a86-4d2c5e68b8d4",
   "metadata": {},
   "source": [
    "Conditional probability tells us that, given some condition, this is the probability of an event happening. We will use weather as an example. Given that <i>B</i> is the condition that it's cloudy outside and <i>A</i>, is the event of rain, conditional probability would then ask, what is the probability of rain, <i>A</i>, given that it's cloudy outside, <i>B</i>.\n",
    "\n",
    "Conditional probability looks at a subset of data that meets the condition. For our example, that's the set of data where it's cloudy outside on a given day. Then we can obtain that probability by counting the number of days where it rains and the total number of days it's cloudy (not necessarily rainy). Then our probability is the following:\n",
    "    \n",
    "$$\n",
    "P(Rain|Cloudy)=\\frac{\\text{Total number of rainy days that are also cloudy}}{\\text{Total number of cloudy days (may or may not be raining)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ce493-e0dd-4dd7-afda-fa9889987c62",
   "metadata": {},
   "source": [
    "Mathematically, we can write this generically as follows:\n",
    "$$\n",
    "  P(A|B)=\\frac{P(A\\text{∩}B)}{P(B)} \\text{ if } P(B) > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f3be3-ccea-41be-b796-39d56aeaf82d",
   "metadata": {},
   "source": [
    "For an interactive example, check out this chapter from Brown University:<br>\n",
    "https://seeing-theory.brown.edu/compound-probability/index.html#section3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281dbdc-8284-4b27-952a-fc36e73347c6",
   "metadata": {},
   "source": [
    "### Law of Total Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65dadbf-ac5b-4bbe-a359-c8aef9cec4f4",
   "metadata": {},
   "source": [
    "Let's say we're looking for a total probability of an event, <i>P(A)</i>, but we only have probability of A given it's intersection with several other events, <i>P(A∩B)</i> and <i>P(A∩B<sup>C</sup>)</i>. The image below shows <i>P(A)</i> which we are trying to find."
   ]
  },
  {
   "attachments": {
    "0efe6815-c11c-4a5a-af52-e34d3d787245.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6ddf1af5-c7f4-4ace-8a17-461afb8eb6fb",
   "metadata": {},
   "source": [
    "![total-probability-rule.png](attachment:0efe6815-c11c-4a5a-af52-e34d3d787245.png)\n",
    "<i>https://www.statisticshowto.com/total-probability-rule/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8af0b-c5f4-4025-958f-4dd673cc2a7a",
   "metadata": {},
   "source": [
    "The Law of Total Probability assists us in calculating <i>P(A)</i> in this situation. Given that we have all the intersections between <i>A</i> and <i>B<sub>i</sub></i> events, we can take the sum of those events:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7a475-8009-460a-a248-5c3a0f7ca55e",
   "metadata": {},
   "source": [
    "$$\n",
    "  P(A)= \\sum_{i} P(A\\text{∩}B_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6815431-7c0e-4733-907e-22942fd7befe",
   "metadata": {},
   "source": [
    "This rule can also be rewritten given the conditional probability. To remove the conditional, we need to multiply by the probability of the event. Looking back on the conditional probability formula, this should make sense to you.\n",
    "$$\n",
    "  P(A)= \\sum_{i} P(A\\text{∩}B_i)= \\sum_{i} P(A|B_i) P(B_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510450d7-0eca-4915-a242-749d895e6a1a",
   "metadata": {},
   "source": [
    "### Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a3a37-ec99-4359-ab13-6ca8fdd6b742",
   "metadata": {},
   "source": [
    "Bayes Rule is a combination of the Law of Total Probability and Conditional Probability. In the simplist terms, Bayes Theorem answers the question, \"Given <i>this</i>, what is the probability of <i>this other thing</i> occuring?\" An example of this would be: given I have a fever, what is the probability I have covid? We represent it as follows:<br>\n",
    "$$\n",
    "P(Covid|Fever)=\\frac{P(Fever|Covid)P(Covid)}{P(Fever)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec43bb-8c1d-42dc-adb2-a0b5cb0510d1",
   "metadata": {},
   "source": [
    "More generally we can write Bayes Rule as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d27252-36f7-46f9-86a3-ba6e37764fc3",
   "metadata": {},
   "source": [
    "$$\n",
    "  P(X=x|Y=y) = \\frac{P(X=x \\text{∩} Y=y)}{P(Y=y)}= \\frac{P(X=x)P(Y=y|X=x)}{\\sum_{x'} P(X=x')P(Y=y|X=x')}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699ba07-a979-45c2-9d58-c8fb5595067d",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83a9c0-1525-46db-a040-a71a99f85a21",
   "metadata": {},
   "source": [
    "**How does probability tie into machine learning?** Well, in practice, we rarely know any of the probabilities to do these calculations. What happens instead is we take a large sample of data and make approximations using supervised learning techniques. \n",
    "\n",
    "<b>Supervised learning</b> means we know the before and after, or, in other words, we know both our inputs into the algorithm as well as our outputs for learning.<br>\n",
    "\n",
    "We can use a generative modeling approach to approximate *P(A)* and *P(B|A)*, then follow up with Bayes rule to compute *P(A|B)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91516740-6c61-4954-9499-fd5825440bd1",
   "metadata": {},
   "source": [
    "## Example\n",
    "*This example was derived from Machine Learning an Algorithmic Perspective 2nd Edition by Stephen Marshland*\n",
    "\n",
    "Features/Inputs:\n",
    "$$\n",
    "\\newcommand\\mycolv[1]{\\begin{bmatrix}#1\\end{bmatrix}}\n",
    "x = \\mycolv{x_1\\\\.\\\\.\\\\.\\\\x_n}\n",
    "$$\n",
    "\n",
    "Targets/Labels: \n",
    "$$\n",
    "c = \\mycolv{c_1\\\\.\\\\.\\\\.\\\\c_n}\n",
    "$$\n",
    "\n",
    "Definitions:\n",
    "\n",
    "**Prior Probability P(c<sub>i</sub>)**: Probability of a class without seeing any data.\n",
    "\n",
    "**Class-Conditional Probability P(x|c<sub>i</sub>)**: Probability of data (features) given a class.\n",
    "\n",
    "**Posterior Probability P(c<sub>i</sub>|x)**: Probability of a class given features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772dd60-4325-476d-96cb-b6338258e5b5",
   "metadata": {},
   "source": [
    "### Predict what activity *c<sub>i</sub>* you will do given 3 features *x*.\n",
    "\n",
    "|Deadline?/x<sub>1</sub>|Is There a Party?/x<sub>2</sub>|Lazy?/x<sub>3</sub>|Activity/c|\n",
    "|---|---|---|---|\n",
    "|Urgent|Yes|Yes|Party|\n",
    "|Urgent|No|Yes|Study|\n",
    "|Near|Yes|Yes|Party|\n",
    "|None|Yes|No|Party|\n",
    "|None|No|Yes|Pub|\n",
    "|None|Yes|No|Party|\n",
    "|Near|No|No|Study|\n",
    "|Near|No|Yes|TV|\n",
    "|Near|Yes|Yes|Party|\n",
    "|Urgent|No|No|Study|\n",
    "\n",
    "To put this into perspective, our input vector, *x*, for a sample is a single row of data excluding the last column. The last column in the row is our label or class, *c*, for the sample. The first row contains the input:\n",
    "$$\n",
    "x = \\mycolv{x_1=Urgent\\\\x_2=Yes\\\\x_3=Yes}\n",
    "$$\n",
    "\n",
    "And the output:\n",
    "$$\n",
    "c = \\mycolv{Party}\n",
    "$$\n",
    "\n",
    "From the data above, we can collect the following probabilities:\n",
    "$$\n",
    "P(c=party)=\\frac{5}{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(c=study)=\\frac{3}{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(c=TV)=\\frac{1}{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(c=Pub)=\\frac{1}{10}\n",
    "$$\n",
    "\n",
    "Assume we only have the feature *x<sub>1</sub>* where c=party (5 total examples):\n",
    "\n",
    "|Urgent|Near|None|\n",
    "|---|---|---|\n",
    "|1|2|2|\n",
    "\n",
    "$$\n",
    "P(x_1=Urgent|c=Party)=\\frac{1}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Near|c=Party)=\\frac{2}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=None|c=Party)=\\frac{2}{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dcb041-d6c2-4517-8aeb-1d66443084be",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *This section won't be reviewed and is just for you to practice this concept*\n",
    "\n",
    "**Repeat the above for each class c<sub>i</sub> to finish training classifier**\n",
    "<details>\n",
    "    <summary>Open Pratice</summary>\n",
    "$$\n",
    "P(x_1=Urgent|c=Study)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Near|c=Study)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=None|c=Study)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Urgent|c=Pub)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Near|c=Pub)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=None|c=Pub)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Urgent|c=TV)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Near|c=TV)=\\frac{}{}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=None|c=TV)=\\frac{}{}\n",
    "$$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed025cb-db44-4c22-92b1-414f7752810f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### *Making a Prediction*\n",
    "We can make a prediction on a new test example using our MAP rule which in this case is Bayes rule:\n",
    "$$\n",
    "P(c_i|x_1)=\\frac{P(x_1|c_i)P(c_i)}{P(x_1)}\n",
    "$$\n",
    "\n",
    "By law of total probability we can sub *P(x<sub>1</sub>)* for:\n",
    "$$\n",
    "P(c_i|x_1)=\\frac{P(x_1|c_i)P(c_i)}{\\sum_{j} P(x_1|c_j) P(c_j)}\n",
    "$$\n",
    "\n",
    "If you do the excercise above left for you, you can sub in those values to this equation to find any combination of P(c<sub>i</sub>=[Party,Study,TV,Pub] | x<sub>1</sub>=[Urgent,Near,None])\n",
    "\n",
    "Lets add in feature *x<sub>3</sub>* where c=party.\n",
    "\n",
    "| | |Urgent|Near|None|\n",
    "|---|---|---|---|---|\n",
    "|Lazy?|Yes|1|2|0|\n",
    "|Lazy?|No|0|0|2|\n",
    "\n",
    "Then we can obtain the following probabilities:\n",
    "$$\n",
    "P(x_1=Urgent, x_3=Yes|c=Party)=\\frac{1}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Urgent, x_3=No|c=Party)=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Near,x_3=Yes|c=Party)=\\frac{2}{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=Near,x_3=No|c=Party)=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=None, x_3=Yes|c=Party)=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x_1=None, x_3=No|c=Party)=\\frac{2}{5}\n",
    "$$\n",
    "\n",
    "Note that as the number of features increases, so does the number of empty bins above. If we had more training data, some of these bins may become greater than 0. If we have *d* features, we have ~2<sup>d</sup> bins.\n",
    "    \n",
    "However, to have all non-zero bins (best case) our training data must grow exponentially with *d*. It's impossible to obtain this much data for a large amount of features so **machine learning algorithms make assumptions about the probabilities or data structure in order to require less training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceb28c-59ca-4d7c-b3eb-91a24b9fc2bc",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbaaa4-7cde-416a-a0c8-9863a3e0fad4",
   "metadata": {},
   "source": [
    "For the Naive Bayes machine learning classifier, **we make the assumption that features are conditionally *independent* given a class**. This obviously isn't true, as there are many real world examples where features are dependent on one another, but making this assumption simplifies the algorithm and gets us really close. This is our original model (dependent):\n",
    "$$\n",
    "P(x_j|c_i)=P(x^1_j,x^2_j,...,x^d_j|c_i)P(c_i)\n",
    "$$\n",
    "And this is our Naive Bayes Assumption (independent):\n",
    "$$\n",
    "P(x_j|c_i)=P(x^1_j|c_i)P(x^2_j|c_i)...P(x^d_j|c_i)\n",
    "$$\n",
    "\n",
    "In turn, our MAP rule becomes:\n",
    "$$\n",
    "P(c_i|x_j)=\\frac{P(x_j|c_i)P(c_i)}{P(x_j)}\\\\\n",
    "\\alpha P(x_j|c_i)P(c_i)\\\\\n",
    "=P(c_i)\\prod^d_{k=1}P(x^k_j|c_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea9b3c-8cc8-4059-aef8-7fcdccaeec25",
   "metadata": {},
   "source": [
    "# EXCERCISE\n",
    "This begins your official interview exercise. Please do your best to fill in and complete this section. There will be some basics introduced here under the assumption you've never used these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5e90b1-a13a-4837-b13c-daf1950ff9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda31bf5-f362-4e72-a829-61d9dabcd3dd",
   "metadata": {},
   "source": [
    "##### 1) Make an array *a* using numpy of size 8x5 where every element is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627bfef8-2706-400f-99ce-bed0a0f7da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44a2a4fa-c0a5-4b2f-80d1-3201f57601fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f723ed-516e-44f6-a04d-5bb7becb2d01",
   "metadata": {},
   "source": [
    "##### 2) Make an array *b* of size 8x5 that has 4 on the leading diagonal and 1 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe7502a-f229-4dfa-b042-c3a7caf0462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f31b165-c0d5-48f2-92d5-f7b7794b083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 1., 1., 1., 1.],\n",
       "       [1., 5., 1., 1., 1.],\n",
       "       [1., 1., 5., 1., 1.],\n",
       "       [1., 1., 1., 5., 1.],\n",
       "       [1., 1., 1., 1., 5.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc7e71-756d-4cae-86f0-99283b776be5",
   "metadata": {},
   "source": [
    "##### 3) Can you multiply these two matrices together? Why does a*b work, but not dot(a,b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b97a8eb-50b1-4faa-bc42-57c6be0ab18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32bd9d9-f702-47d4-91af-eca08b10953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224366e-3991-456e-bce6-f9de72758202",
   "metadata": {},
   "source": [
    "*Write your explanation here*\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027e8c2-2f01-463b-8a66-4100192e602b",
   "metadata": {},
   "source": [
    "##### 4) Your first step in machine learning is to read in the data which you would like to train your classifier. We've provided this dataset in the repo, *dataset.csv*. Bring this data into the notebook and store it as a numpy matrix using pandas to read in the data. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\">read_csv</a> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59a4dfb-1920-4b81-a653-1322854ddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b8bb9-faf5-44fc-bc5c-c5cfba623f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before you move on, you may want to preview your dataset to see what you're working with. You'll want to use numpy for your dataset manipulation but pandas has a cleaner print function. Try using it to preview your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23f035c4-1a27-4b0a-81a3-891a589659df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.816094</td>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.219718</td>\n",
       "      <td>0.031388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.823072</td>\n",
       "      <td>0.514420</td>\n",
       "      <td>0.240063</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.807796</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.237587</td>\n",
       "      <td>0.031678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.440035</td>\n",
       "      <td>0.248716</td>\n",
       "      <td>0.057396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798370</td>\n",
       "      <td>0.557353</td>\n",
       "      <td>0.225954</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.808466</td>\n",
       "      <td>0.522134</td>\n",
       "      <td>0.269489</td>\n",
       "      <td>0.033686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.805978</td>\n",
       "      <td>0.521515</td>\n",
       "      <td>0.268659</td>\n",
       "      <td>0.079017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.766939</td>\n",
       "      <td>0.571445</td>\n",
       "      <td>0.285722</td>\n",
       "      <td>0.060152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.825123</td>\n",
       "      <td>0.528079</td>\n",
       "      <td>0.198030</td>\n",
       "      <td>0.033005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_0  Feature_1  Feature_2  Feature_3  Target\n",
       "0   0.816094   0.533600   0.219718   0.031388       0\n",
       "1   0.823072   0.514420   0.240063   0.017147       0\n",
       "2   0.807796   0.538530   0.237587   0.031678       0\n",
       "3   0.860939   0.440035   0.248716   0.057396       0\n",
       "4   0.798370   0.557353   0.225954   0.030127       0\n",
       "5   0.808466   0.522134   0.269489   0.033686       0\n",
       "6   0.805978   0.521515   0.268659   0.079017       0\n",
       "7   0.766939   0.571445   0.285722   0.060152       0\n",
       "8   0.825123   0.528079   0.198030   0.033005       0\n",
       "9   0.790965   0.569495   0.221470   0.031639       0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10) #df is the variable name I used when I read in my data with pandas. Replace it with your pandas dataframe variable name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2d9d8-e81a-4444-b9f2-d2e7c83ecb79",
   "metadata": {},
   "source": [
    "##### 5) Next, the data needs shuffled. Shuffling the data helps reduce bias when training. We don't want to have our training set missing any labels! Use numpy to shuffle the rows within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82b8746-6a31-47ff-83d2-e6b0c7e32098",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "#good practice to seed before shuffling so that when you come back to work on your problem, \n",
    "#you get the same consistent shuffle the next time. Otherwise, if you close the notebook and go to rerun later,\n",
    "#you won't get the same results. When we are trying to generalize the model, we want our data to be mixed\n",
    "#but consistent too otherwise you will have leaking between your training and test which isn't good for finding a good generalized model\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bb797-98ca-4cb2-8fed-e5a163b3c724",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 6) We need to split the dataset into a training and test set. The training data is what you will give to the classifier for it to learn from. The test set inputs will then be driven into the trained classifier so we can compare the results that the classifier gives to the actual output labels. Create a numpy array *training* and *test* where *training* contains 80% of the dataset and *test* contains 20% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c031a512-d3bb-4552-90aa-add4ef0c8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "#lastly, check the shape of test to make sure we didn't lose any data (total between train and test should be 124)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda236d-f06f-4e46-92db-70f69c2e7699",
   "metadata": {},
   "source": [
    "The dataset here contains features in the first 4 columns (0 through 3, inclusive). \n",
    "\n",
    "In the very last column is our class labels, [0,1,2]. Since we've obscured the dataset before giving it to you, you can give these any context you'd like. For instance, 0 could identify the color red, 1 could identify the color blue, and 2 could identify the color green; we've just given the real label a numerical representation that can be mapped back at a later time. \n",
    "\n",
    "This same logic also applies to the feature columns which were transformed using a method called normalization. Normalizing the data makes the scale more uniform and generally makes machine learning algorithms perform better.\n",
    "\n",
    "##### 7) Both the *training* data and *test* data need split column wise such that we have four numpy arrays, *training_features*, *training_labels*, *test_features*, and *test_labels*. This is done so that we can feed the algorithm the inputs (features) and outputs (labels, the thing we can to guess) separately. \n",
    "*Hint: Make sure your training and test label arrays have dimensions (100,1) and (24,1)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ce0ab-90da-4880-8593-a74e153b4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "training_features = \n",
    "training_labels   = \n",
    "test_features     = \n",
    "test_labels       = \n",
    "\n",
    "#Check dimensions (For math things later)\n",
    "print(training_features.shape)\n",
    "print(training_labels.shape)\n",
    "print(test_features.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed95430-8b23-492e-97c2-bcf8f4066f28",
   "metadata": {},
   "source": [
    "##### 8) Now that we have our data prepared, we can feed it into our Naive Bayes Classifier to be trained! Oh, but we need a classifier first. Build a Naive Bayes Classifier by filling in the methods below (you may add methods, additional parameters, and class variables as you see fit):\n",
    "*Hint: We no longer have a discerete set of values for our features, which makes this a bit different from our initial example. Since this data set is continuous, we make an alterations to our probability function by using a <a href=\"https://scikit-learn.org/stable/modules/naive_bayes.html\">Guassian Distribution</a>*\n",
    "$$\n",
    "P(x_i|c)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_c}}\\text{exp}(-\\frac{(x_i-\\mu_c)^2}{2\\sigma^2_c})\n",
    "$$\n",
    "Where σ is the standard deviation and µ is the mean of a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4643c16d-8024-49bc-a983-9b14899f7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit this class however you please. We gave it some structure to make problem solving a bit quicker on your end.\n",
    "#If you envision the setup a certain way and can improve on it, go for it!\n",
    "#As long as the algorithm works and the class is well documented, that's good enough for us!\n",
    "\n",
    "class NaiveBayes:\n",
    "    means = {} #means of each classes features\n",
    "    stdDev = {} #standard deviations of each classes features\n",
    "    labels = {} #Our unique labels\n",
    "    rowsInSet = {} #Dictionary containing how many of each label we have\n",
    "    classProb = {} #Contains the probability of every class occurence\n",
    "    \n",
    "    def splitByLabel(self, features, labels):\n",
    "        #Get the subsets of data for each unique class/label\n",
    "        #return the dictionary of those sets\n",
    "        datasetsByLabel = {} #Dictionary containing all the subsets\n",
    "        self.rowsInSet = {} #Dictionary containing how many of each label we have (i.e. the frequency of appearance or num of rows in subset)\n",
    "        \n",
    "        return datasetsByLabel\n",
    "    \n",
    "    def guassianProbabilityCalc(self, features, mean, stdDev):\n",
    "        #Calculate class probability using guassian distribution\n",
    "        #1/sqrt(2*pi*(mean_c)^2)e^(-(x_i-stdDev_c)^2/(2*(mean_c)^2)) see above cell\n",
    "\n",
    "        return leadingTerm*expComponent\n",
    "        \n",
    "    def getClassProbability(self, inputFeatureVector):\n",
    "        #predict the probability of every class for the given input (a single row of data with 4 features)\n",
    "        #Reference the MAP product rule\n",
    "        classProbInput = {}\n",
    "        \n",
    "        return classProbInput\n",
    "    \n",
    "    def train(self, features, labels):\n",
    "        self.labels = np.unique(labels) #All of our unique class labels [0,1,2]\n",
    "        #Setups and stores the variables we need to do the MAP rule and make predictions\n",
    "        #Revisit the example and the guassian probability equation if you are stuck.\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict(self, features):\n",
    "        #Predict the labels for the inputs\n",
    "        output = [] #All of the output labels\n",
    "        for x in features: \n",
    "        #Go through each row of data, make a prediction, append label to output \n",
    "        #(it should look like test_labels when you are done)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49603662-6f57-422a-87d0-3623f9ebf6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()\n",
    "model.train(training_features, training_labels)\n",
    "model.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3ec42-92e3-48e1-a3a4-3792250fede1",
   "metadata": {},
   "source": [
    "##### 9)Now that you've made your predictions, let's look at how our classifier performed. Use 3 of <a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html\">scikit learn's classifier metrics</a> to evaluate how your classifier performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d489b7-30fe-43ad-9cfc-179452617fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40d8f4-b0fa-45d0-9645-e42d62f95eaa",
   "metadata": {},
   "source": [
    "##### 10) Did it perform how you expected? Or di exceed your expectations? Try to improve the performance of your classifier if it isn't performing as expected or, if it did perform well, justify it's performance using data visualizations. This may require some <a href=\"https://towardsdatascience.com/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee\">*exploratory data analysis*</a>. Use visualizations, statistics, and whatever else you need to try to improve performance or support your work. Leave your work and documentation below (use as many cells as you need). You may use libraries such as Plotly, Matplotlib, etc. to aid you. There is no right or wrong answers here, just exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f06c3-c1ae-4ef2-aa6d-c11732f53a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95291124-092e-414f-8dfd-40d8c1dafc05",
   "metadata": {},
   "source": [
    "##### 11) Were you able to make improvements to your classifier? Why or why not? Hint: Naive Bayes is generally pretty straight forward to plot and see where the algorithm created boundaries. Use this to help you come to your conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87262167-35b6-4223-a793-186ce738aba0",
   "metadata": {},
   "source": [
    "*Your explanation and supporting content here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
